# ğŸš€ LLM Checker GUI - Standalone Windows Application

Application Windows standalone qui dÃ©tecte votre hardware et recommande les meilleurs modÃ¨les Ollama LLM pour votre systÃ¨me.

## ğŸ¯ FonctionnalitÃ©s

- âœ… **DÃ©tection Hardware Automatique**
  - CPU (marque, cÅ“urs, instructions SIMD)
  - GPU NVIDIA (via nvidia-smi)
  - GPU AMD (via rocm-smi)
  - MÃ©moire systÃ¨me et VRAM

- âœ… **Synchronisation en Ligne** ğŸŒ **OBLIGATOIRE**
  - Synchronisation automatique au dÃ©marrage depuis ollama.com
  - **Toujours Ã  jour** avec les derniers modÃ¨les Ollama !
  - Bouton "Sync Models" pour re-synchroniser manuellement
  - Indicateur de fraÃ®cheur des donnÃ©es
  - **âš ï¸ Connexion Internet requise**

- âœ… **Recommandations Intelligentes**
  - Score multi-dimensionnel (Quality, Speed, Fit, Context)
  - Top 5 modÃ¨les adaptÃ©s Ã  votre hardware
  - Estimation de vitesse (tokens/seconde)
  - VÃ©rification de compatibilitÃ© mÃ©moire

- âœ… **Interface Utilisateur Simple**
  - Tkinter GUI propre et lisible
  - 7 use cases : general, coding, reasoning, chat, creative, fast, quality
  - Bouton Refresh pour relancer la dÃ©tection
  - Bouton Sync pour mettre Ã  jour la base de modÃ¨les
  - Affichage des commandes `ollama pull` Ã  copier

- âœ… **Standalone & Simple**
  - Un seul fichier .exe
  - Pas besoin de Python installÃ©
  - Toutes les dÃ©pendances incluses
  - **Connexion Internet requise** (synchronisation en temps rÃ©el)
  - Taille optimisÃ©e (15-25 MB)

## ğŸ“¸ Capture d'Ã‰cran

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸš€ LLM Checker - Ollama Model Recommender         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  ğŸ’» Hardware Detected                               â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•— â”‚
â”‚  â•‘ System: NVIDIA RTX 4090 (24GB VRAM) + 16 coresâ•‘ â”‚
â”‚  â•‘ Tier: VERY HIGH                                â•‘ â”‚
â”‚  â•‘ Backend: NVIDIA CUDA                           â•‘ â”‚
â”‚  â•‘ Max Model Size: 22.0 GB                        â•‘ â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚                                                      â”‚
â”‚  â­ Top 5 Recommended Models                        â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•— â”‚
â”‚  â•‘ 1. qwen2.5-coder:14b-instruct-q8_0            â•‘ â”‚
â”‚  â•‘    Score: 98/100 â­                            â•‘ â”‚
â”‚  â•‘    Quality: 95 | Speed: 100 | Fit: 98         â•‘ â”‚
â”‚  â•‘    ~125 tokens/sec                             â•‘ â”‚
â”‚  â•‘    ollama pull qwen2.5-coder:14b-instruct-q8_0â•‘ â”‚
â”‚  â•‘                                                 â•‘ â”‚
â”‚  â•‘ 2. llama3.3:70b-instruct-q4_k_m               â•‘ â”‚
â”‚  â•‘    ...                                          â•‘ â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚                                                      â”‚
â”‚  Use Case: [coding â–¼]  [ğŸ”„ Refresh] [ğŸŒ Sync]       â”‚
â”‚  ğŸ“… Last sync: 2h ago                    âœ… Ready   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸƒ Quick Start (Pour Utilisateurs)

1. **TÃ©lÃ©charger `LLM_Checker.exe`**
2. **VÃ©rifier que vous avez Internet** (obligatoire)
3. **Double-cliquer** sur le fichier
4. **L'app synchronise automatiquement** les modÃ¨les depuis ollama.com (~30 sec)
5. **Voir les rÃ©sultats** immÃ©diatement !

C'est tout ! Aucune installation requise.

**âš ï¸ IMPORTANT:** Une connexion Internet est **obligatoire** car l'application synchronise toujours les derniers modÃ¨les depuis ollama.com Ã  chaque dÃ©marrage. Cela garantit que vous avez toujours les modÃ¨les les plus rÃ©cents.

## ğŸ› ï¸ Build Instructions (Pour DÃ©veloppeurs)

Voir [BUILD_INSTRUCTIONS.md](BUILD_INSTRUCTIONS.md) pour compiler le .exe depuis les sources.

**RÃ©sumÃ© rapide:**
```bash
# Installer les dÃ©pendances
pip install -r requirements.txt

# Tester l'application
python llm_checker_gui.py

# CrÃ©er le .exe
pyinstaller llm_checker.spec

# Le .exe sera dans dist/LLM_Checker.exe
```

## ğŸ“ Structure du Projet

```
python_gui/
â”œâ”€â”€ llm_checker_gui.py       # Application principale (GUI)
â”œâ”€â”€ hardware_detector.py     # Module de dÃ©tection hardware
â”œâ”€â”€ model_scorer.py          # Moteur de scoring des modÃ¨les
â”œâ”€â”€ model_database.py        # Base de donnÃ©es fallback (offline)
â”œâ”€â”€ ollama_sync.py          # Module de synchronisation en ligne ğŸŒ
â”œâ”€â”€ requirements.txt         # DÃ©pendances Python
â”œâ”€â”€ llm_checker.spec         # Configuration PyInstaller
â”œâ”€â”€ build_exe.bat            # Script de build automatique
â”œâ”€â”€ README.md                # Ce fichier
â”œâ”€â”€ BUILD_INSTRUCTIONS.md    # Instructions de build dÃ©taillÃ©es
â””â”€â”€ QUICK_START.txt          # Guide rapide
```

## ğŸ® Utilisation

### Use Cases Disponibles

- **general** : Usage gÃ©nÃ©ral, Ã©quilibrÃ©
- **coding** : OptimisÃ© pour la programmation
- **reasoning** : ModÃ¨les de raisonnement avancÃ©
- **chat** : Conversation fluide et rapide
- **creative** : Ã‰criture crÃ©ative
- **fast** : Vitesse maximale
- **quality** : QualitÃ© maximale

### Comprendre les Scores

Chaque modÃ¨le reÃ§oit 4 scores (0-100):

- **Quality (Q)** : QualitÃ© du modÃ¨le (famille, paramÃ¨tres, quantization)
- **Speed (S)** : Vitesse estimÃ©e en tokens/seconde
- **Fit (F)** : Utilisation optimale de la mÃ©moire disponible
- **Context (C)** : Longueur du contexte supportÃ©

**Score Final** = Moyenne pondÃ©rÃ©e selon le use case

### Exemple de RÃ©sultats

Pour un systÃ¨me avec RTX 3060 (12GB VRAM):

```
Top 5 for 'coding':
1. qwen2.5-coder:7b-instruct-q8_0 - Score: 100/100
2. qwen2.5-coder:7b-instruct-q6_k - Score: 98/100
3. deepseek-coder-v2:16b-q4_k_m - Score: 95/100
4. llama3.1:8b-instruct-q8_0 - Score: 92/100
5. phi4:14b-q6_k - Score: 90/100
```

## ğŸ”§ Modules Python

### 1. Hardware Detector (`hardware_detector.py`)

DÃ©tecte:
- CPU (marque, cÅ“urs, frÃ©quence, SIMD)
- GPU NVIDIA (via `nvidia-smi`)
- GPU AMD (via `rocm-smi`)
- MÃ©moire systÃ¨me
- Calcule le backend optimal et la taille max de modÃ¨le

### 2. Model Scorer (`model_scorer.py`)

ImplÃ©mente:
- SystÃ¨me de scoring multi-dimensionnel
- PondÃ©ration par use case
- Estimation de tokens/seconde
- Filtrage par compatibilitÃ© mÃ©moire

### 3. Ollama Sync (`ollama_sync.py`) ğŸŒ

Synchronisation en ligne **OBLIGATOIRE**:
- Fetch modÃ¨les depuis ollama.com/library Ã  chaque dÃ©marrage
- Parse HTML pour extraire tous les modÃ¨les et variantes
- Cache SQLite local (~/.llm_checker/models_cache.db) pour optimisation
- Retry logic et gestion d'erreurs robuste
- Garantit toujours les donnÃ©es les plus rÃ©centes

### 4. Model Database (`model_database.py`)

âš ï¸ **DEPRECATED** - Fichier conservÃ© uniquement pour rÃ©fÃ©rence:
- Ancienne base de donnÃ©es hardcodÃ©e (80+ modÃ¨les)
- Non utilisÃ© en mode production (app online-only)
- Peut servir pour dÃ©veloppement/tests offline

## ğŸ“Š CompatibilitÃ©

### Hardware SupportÃ©

- âœ… **CPU uniquement** (n'importe quel processeur moderne)
- âœ… **NVIDIA GPU** (GTX 1000+, RTX 2000+, Data Center)
- âœ… **AMD GPU** (RX 6000+, RX 7000+, avec ROCm)
- âœ… **Intel GPU** (dÃ©tection basique)

### OS SupportÃ©

- âœ… **Windows 7/8/10/11** (64-bit)
- âœ… **Connexion Internet** (obligatoire)
- âš ï¸ Linux/Mac : Peut fonctionner avec Python, mais .exe est Windows-only

## ğŸ¤ Contribution

1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/AmazingFeature`)
3. Commit vos changements (`git commit -m 'Add some AmazingFeature'`)
4. Push vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

## ğŸ“ BasÃ© sur

Ce projet est une version GUI standalone du projet [llm-checker](https://github.com/Pavelevich/llm-checker) qui est un outil CLI Node.js.

**DiffÃ©rences:**
- âœ… GUI Windows au lieu de CLI
- âœ… Python au lieu de Node.js
- âœ… .exe standalone (pas besoin d'installation)
- âœ… **Synchronisation automatique** Ã  chaque dÃ©marrage ğŸŒ
- âœ… **Toujours Ã  jour** - tous les modÃ¨les Ollama en temps rÃ©el
- âš ï¸ **Connexion Internet requise** (online-only, pas de mode offline)

## ğŸ“„ License

MIT License - voir le fichier LICENSE du projet principal

## ğŸ™ Remerciements

- [Pavelevich](https://github.com/Pavelevich) pour le projet [llm-checker](https://github.com/Pavelevich/llm-checker) original
- [Ollama](https://ollama.ai) pour les modÃ¨les LLM
- CommunautÃ© Python pour psutil et PyInstaller

---

**Fait avec â¤ï¸ et Python**
